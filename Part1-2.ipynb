{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60402f5c",
   "metadata": {},
   "source": [
    "### CSCN8020-25F-Sec1-Reinforcement Learning Programming\n",
    "### Assignment one \n",
    "#### Name : Haysam Elamin\n",
    "#### StID : 8953681"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd38be7",
   "metadata": {},
   "source": [
    "### Problem 1 – Pick-and-Place Robot (MDP Formulation)\n",
    "\n",
    "We model the repetitive **pick-and-place task** of a robot arm as a **Markov Decision Process (MDP)**:\n",
    "\n",
    "### 1. States (S)\n",
    "We define simple, intuitive states that represent stages of the task:\n",
    "\n",
    "- **Idle** – Robot is waiting at start position.  \n",
    "- **Hold Object** – Robot has successfully gripped the object.  \n",
    "- **Moving** – Robot is carrying the object toward the goal location.  \n",
    "- **End Goal** – Robot has reached the target location with the object.  \n",
    "- **Release** – Robot has placed the object at the goal (terminal state).  \n",
    "\n",
    "Formally:  \n",
    "\n",
    "$$\n",
    "S = \\{\\text{Idle}, \\text{HoldObject}, \\text{Moving}, \\text{EndGoal}, \\text{Release}\\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Actions (A)\n",
    "The robot can perform high-level actions to transition between states:\n",
    "\n",
    "- **Move Toward Object** – Navigate to the object’s location.  \n",
    "- **Pickup** – Close the gripper to grasp the object.  \n",
    "- **Move to Goal** – Carry the object toward the goal.  \n",
    "- **Release** – Open the gripper to drop the object.  \n",
    "- **Stop** – Terminate the task.  \n",
    "\n",
    "Formally:  \n",
    "\n",
    "$$\n",
    "A = \\{\\text{MoveTowardObject}, \\text{Pickup}, \\text{MoveToGoal}, \\text{Release}, \\text{Stop}\\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Transition Model (P)\n",
    "Transitions are **stochastic**, representing uncertainty (slips, failures).  \n",
    "Example probabilities:\n",
    "\n",
    "- From **Idle**:  \n",
    "  - MoveTowardObject → HoldObject (0.90), Idle (0.10)  \n",
    "  - Pickup / MoveToGoal / Release → Idle (1.00)  \n",
    "  - Stop → Release (1.00, terminal)\n",
    "\n",
    "- From **HoldObject**:  \n",
    "  - MoveToGoal → Moving (0.90), HoldObject (0.08), Idle (0.02)  \n",
    "  - Release → Idle (0.98), HoldObject (0.02)  \n",
    "\n",
    "- From **Moving**:  \n",
    "  - MoveToGoal → Moving (0.90), HoldObject (0.05), Idle (0.05)  \n",
    "  - Release → EndGoal (0.95), Moving (0.05)  \n",
    "\n",
    "- From **EndGoal**:  \n",
    "  - Release → Release (0.98, terminal), EndGoal (0.02)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Reward Function (R)\n",
    "Rewards encourage **fast and smooth** completion:\n",
    "\n",
    "- **+10** for successfully releasing the object at the goal.  \n",
    "- **–1** penalty per step (to encourage speed).  \n",
    "- **–5** penalty for invalid actions (e.g., trying to release with no object).  \n",
    "- **0** otherwise.  \n",
    "\n",
    "Formally:  \n",
    "\n",
    "$$\n",
    "R(s,a,s') =\n",
    "\\begin{cases}\n",
    "+10, & \\text{if task completed (Release at EndGoal)} \\\\\n",
    "-5, & \\text{if invalid action taken} \\\\\n",
    "-1, & \\text{each time step (delay penalty)} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Discount Factor (γ)\n",
    "We choose:  \n",
    "\n",
    "$$\n",
    "\\gamma = 0.95\n",
    "$$\n",
    "\n",
    "This ensures the agent values completing the task quickly rather than delaying.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary\n",
    "This MDP captures the essential logic of a pick-and-place robot:  \n",
    "- States track the **progress stage** (Idle → Pick → Move → Goal → Release).  \n",
    "- Actions represent **key operations** (Move, Pickup, Place).  \n",
    "- The stochastic transition model reflects real-world uncertainty.  \n",
    "- Rewards enforce the requirement for **fast and smooth movements**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e20d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Episode Trace (s, a, s', r):\n",
      "(Idle      , MoveTowardObject) -> (HoldObject)   r = -1.0\n",
      "(HoldObject, MoveToGoal      ) -> (Moving    )   r = -1.0\n",
      "(Moving    , Release         ) -> (EndGoal   )   r = -1.0\n",
      "(EndGoal   , Release         ) -> (Release   )   r = +10.0\n",
      "\n",
      "First Episode: steps = 4, return = 7.0, success = True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>steps</th>\n",
       "      <th>return</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode  steps  return  success\n",
       "0        1      4     7.0     True\n",
       "1        2      4     7.0     True\n",
       "2        3      4     7.0     True\n",
       "3        4      4     7.0     True\n",
       "4        5      5     6.0     True\n",
       "5        6      4     7.0     True\n",
       "6        7      4     7.0     True\n",
       "7        8      4     7.0     True\n",
       "8        9      4     7.0     True\n",
       "9       10      4     7.0     True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "          episode      steps     return success\n",
      "count   30.000000  30.000000  30.000000      30\n",
      "unique        NaN        NaN        NaN       1\n",
      "top           NaN        NaN        NaN    True\n",
      "freq          NaN        NaN        NaN      30\n",
      "mean    15.500000   4.233333   6.766667     NaN\n",
      "std      8.803408   0.568321   0.568321     NaN\n",
      "min      1.000000   4.000000   5.000000     NaN\n",
      "25%      8.250000   4.000000   7.000000     NaN\n",
      "50%     15.500000   4.000000   7.000000     NaN\n",
      "75%     22.750000   4.000000   7.000000     NaN\n",
      "max     30.000000   6.000000   7.000000     NaN\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAALCpJREFUeJzt3QmcTfX/x/HPMAwpYzdDwyD7WpTQYt+L+FUqokR5oJC/6EeoX1kqtIhWQyWVLEWRXbZsIYoQ2ZdkjG2GzPk/Pt/f797HvTN3yJiZe74zr+fjcczcc8899/u9517nPd/l3BDHcRwBAACwULZgFwAAACC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgC8hg0bJiEhIRn6nHv37jXPGRMTk6HPCyBzIMgAltITvwaAlJY1a9ZIVpX0tcibN6/cfffdMnfu3FTv85VXXpFZs2alaTkBXLvQNNgHgCB68cUXpVSpUsnW33TTTVe9r8GDB8vAgQMlM2jSpIk8+uijol8n98cff8iECRPknnvuke+++06aNWuWqiDzr3/9S9q2bZsu5QWQOgQZwHItWrSQWrVqpcm+QkNDzZIZlCtXTjp27Oi93b59e6lUqZK88cYbqQoy6SExMVEuXLgguXLlCnZRAGvRtQRkcp4xKK+99pqMHTtWSpYsKblz5zZdLVu3br3iGJkFCxbIHXfcIfny5ZPrr79eypcvL88//7zfNseOHZOuXbtK0aJFzUm5evXqMnny5GRliY2NlS5dukh4eLjZX+fOnc26QLZv325aQAoUKGD2qWHt66+/TvXrULFiRSlUqJDs3r3bb31CQoIMHTrUtGCFhYVJVFSUDBgwwKz30Nfk7Nmzpk6e7iqth9Kf0dHRyZ4v0Gupt3v16iWffvqpVK5c2TzfvHnzvN2EK1eulH79+knhwoUlT548ct9998nx48f99rF+/XoTxLQuehy1Ne7xxx9P9esC2C5z/OkFZGGnTp2SP//802+dnhQLFizot27KlCly+vRp6dmzp8THx5uWiYYNG8rPP/9sAkgg27Ztk9atW0u1atVMF5aeeHft2mVOuB7nz5+X+vXrm/V6ktYT65dffmlO8BpSnnnmGbOddvG0adNGVqxYIU899ZQJFjNnzjRhJtDz1qtXT4oXL266uvSk/sUXX5huna+++sqc4FPzOp08eVLKlCnj1yJy7733mjJ1797dlElfDw18v/32m3dMzMcffyxPPPGE3HbbbWY75bufq7F48WJTF32tNIxoCNq0aZO5r3fv3pI/f34TrDSAjhs3zmz3+eefewNj06ZNTdDR10XDoG43Y8aMVJUFyBQcAFaaNGmSox/hQEtYWJh3uz179ph1uXPndg4cOOBd/+OPP5r1ffv29a4bOnSoWecxduxYc/v48eMplmPcuHFmm08++cS77sKFC06dOnWc66+/3omLizPrZs2aZbYbPXq0d7u///7bufPOO816rY9Ho0aNnKpVqzrx8fHedYmJiU7dunWdsmXLXvG10f117drVlPvYsWPO+vXrnebNm5v1r776qne7jz/+2MmWLZvzww8/+D1+4sSJZtuVK1d61+XJk8fp3LlzsufSdSVLlky2Pulr6SmXPt+2bdsCHsvGjRubenroscmePbsTGxtrbs+cOdNst27duiu+BkBWQdcSYLnx48eb7h/fRQe0JqWtGdrC4aGtC7Vr15Zvv/02xX3rX/xq9uzZpvUiEH18RESEPPTQQ951OXLkkKefflrOnDkjy5Yt826n42969Ojh3S579uymFcLXX3/9ZVotHnjgAdOCpK1Nupw4ccJ0qezcuVMOHjx4xdflww8/NC0XRYoUMd1SixYtMl1G2nXjoS1H2gpToUIF7/Pooi1VasmSJZLWtEtPx+oEoq09vt1Rd955p1y6dMkMVvY9HnPmzJGLFy+medkAG9G1BFhOA8k/GexbtmzZgANitZsjJQ8++KB88MEHpltFuzIaNWok7dq1M2NXsmX7799BepLVfXtue2hA8Nzv+RkZGWnG2fjSMTe+tItKGy+GDBlilkC0i8U3lAWi3VjaLaODadetW2dmHZ07d86vnBqKfv31VxN4UnqetBZohplHiRIl/G5rN5PSLjFPCNJBy8OHDzfdX9qlpwH14YcfNt1+QFZEkAGQIh1Munz5ctMyoddg0YGpOl5DWyy+//5706KS1jwtP/37909xdtE/mVp+4403SuPGjc3vLVu2NONRNNg0aNDAhDHPc1WtWlXGjBkTcB868PdKUrqAoLakpPSapiSl1/O/vVL/fa7p06ebawR98803Mn/+fDPQ9/XXXzfrkoZEICsgyABZhLY+JKUDWgPNuPGlLRjaEqOLnvC1ZePf//63CTcaFHQW1JYtW0wo8G3t0FlHSu/3/NTuHe1u8j3h7tixw+/5Spcu7e2e8gSRtPDkk0+aVgy9Vo4OFtZQoAN2N2/ebOp2pSsap3S/tpoEmnnlaYlKD7fffrtZXn75ZZk6dao88sgjMm3aNNNyBmQ1jJEBsgidgeM7tmTt2rXy448/muvQpETHqyRVo0YN89MzPVlbO44cOeKdWaP+/vtveeutt0xg0e4Qz3a6Xi9M59tqodv50jEt2mXy7rvvyuHDh5M9f9LpyP+Ujs959tlnTVeSjvlROg5HX5P3338/2fY6G0unXHvozKlAgUXDkM6I0jDnoeXWGVlpTbuYPK0zKR0PIKuhRQawnA7s9bR++Kpbt663dcPTHaPXg9HBtnrS06m9OkVbB8CmRKdca9dSq1atTIuKjhl55513TLeN7sszQFVDh0633rBhg2nh0e4PnaKtz3HDDTeY7fSqujqlWsfa6JRhHfCq04Y1BAQawKz7126fbt26mXocPXpUVq9eLQcOHDCtKKmhZXzhhRdk1KhRZmxJp06dzBghnQ6uLUxaPg1X+nrqeu268Yw/qlmzpixcuNC0ShUrVsyMddHB0h06dJDnnnvOtPLoAGcdh6NhTccfbdy4UdKSXsdGX399Lg1QOhhaQ5h+BYMGRSBLCva0KQBpP/3adzqzZ/q1Tjt+/fXXnaioKDM9W6c9b968+bJThhctWuS0adPGKVasmJMzZ07z86GHHnJ+++03v8cdPXrUeeyxx5xChQqZ7XTqtO90ao8TJ044nTp1cvLmzeuEh4eb33/66adk06/V7t27nUcffdSJiIhwcuTI4RQvXtxp3bq1M3369Cu+Nrq/nj17Brxv2LBh5v4lS5Z4p4qPGjXKqVy5snld8ufP79SsWdMZPny4c+rUKe/jtm/f7tx1111mGrs+3ncq9vfff+9UqVLF1L18+fJmKnpK068DlctzLJNOq9Yy+pZ148aN5vUvUaKEKWuRIkXMa6LTy4GsKkT/CXaYApB+tPVDWw9effVVM4AWADITxsgAAABrEWQAAIC1CDIAAMBajJEBAADWokUGAABYiyADAACslekviKeXTT906JC5KNeVLkEOAADcQUe+6EUf9QKUSb+UNksFGQ0x/+SL3wAAgPvs37/fXE08ywYZz+XR9YXQy3gDAAD3i4uLMw0RnvN4lg0ynu4kDTEEGQAA7HKlYSEM9gUAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAawU1yIwYMUJuvfVW8z0KRYoUkbZt28qOHTv8tqlfv765PLHv8tRTTwWtzAAAwD2CGmSWLVsmPXv2lDVr1siCBQvk4sWL0rRpUzl79qzfdt26dZPDhw97l9GjRwetzAAAwD2C+qWR8+bN87sdExNjWmY2bNggd911l3f9ddddJxEREUEoIQAAcDNXjZE5deqU+VmgQAG/9Z9++qkUKlRIqlSpIoMGDZJz584FqYQAAMBNgtoi4ysxMVH69Okj9erVM4HF4+GHH5aSJUtKsWLFZMuWLfLcc8+ZcTQzZswIuJ+EhASzeMTFxWVI+QEAQBYOMjpWZuvWrbJixQq/9d27d/f+XrVqVYmMjJRGjRrJ7t27pUyZMgEHEA8fPjxDygwAQFqKHjhXbLN3ZKugPr8rupZ69eolc+bMkSVLlsiNN9542W1r165tfu7atSvg/dr1pF1UnmX//v3pUmYAAJDFW2Qcx5HevXvLzJkzZenSpVKqVKkrPmbTpk3mp7bMBBIWFmYWAACQ+YUGuztp6tSpMnv2bHMtmSNHjpj14eHhkjt3btN9pPe3bNlSChYsaMbI9O3b18xoqlatWjCLDgAAsnqQmTBhgveid74mTZokXbp0kZw5c8rChQtl3Lhx5toyUVFR0r59exk8eHCQSgwAANwk6F1Ll6PBRS+aBwAA4NrBvgAAAKlBkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtYIaZEaMGCG33nqr3HDDDVKkSBFp27at7Nixw2+b+Ph46dmzpxQsWFCuv/56ad++vRw9ejRoZQYAAO4R1CCzbNkyE1LWrFkjCxYskIsXL0rTpk3l7Nmz3m369u0r33zzjXz55Zdm+0OHDkm7du2CWWwAAOASocF88nnz5vndjomJMS0zGzZskLvuuktOnTolH374oUydOlUaNmxotpk0aZJUrFjRhJ/bb789SCUHAABu4KoxMhpcVIECBcxPDTTaStO4cWPvNhUqVJASJUrI6tWrA+4jISFB4uLi/BYAAJA5uSbIJCYmSp8+faRevXpSpUoVs+7IkSOSM2dOyZcvn9+2RYsWNfelNO4mPDzcu0RFRWVI+QEAQBYOMjpWZuvWrTJt2rRr2s+gQYNMy45n2b9/f5qVEQAAuEtQx8h49OrVS+bMmSPLly+XG2+80bs+IiJCLly4ILGxsX6tMjprSe8LJCwszCwAACDzC2qLjOM4JsTMnDlTFi9eLKVKlfK7v2bNmpIjRw5ZtGiRd51Oz963b5/UqVMnCCUGAABuEhrs7iSdkTR79mxzLRnPuBcd25I7d27zs2vXrtKvXz8zADhv3rzSu3dvE2KYsQQAAIIaZCZMmGB+1q9f32+9TrHu0qWL+X3s2LGSLVs2cyE8nZHUrFkzeeedd4JSXgAA4C6hwe5aupJcuXLJ+PHjzQIAAODKWUsAAABXiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYKapBZvny53HPPPVKsWDEJCQmRWbNm+d3fpUsXs953ad68edDKCwAA3CWoQebs2bNSvXp1GT9+fIrbaHA5fPiwd/nss88ytIwAAMC9QoP55C1atDDL5YSFhUlERESGlQkAANjD9WNkli5dKkWKFJHy5ctLjx495MSJE8EuEgAAcImgtshciXYrtWvXTkqVKiW7d++W559/3rTgrF69WrJnzx7wMQkJCWbxiIuLy8ASAwCAjOTqINOhQwfv71WrVpVq1apJmTJlTCtNo0aNAj5mxIgRMnz48AwsJQAACBbXdy35Kl26tBQqVEh27dqV4jaDBg2SU6dOeZf9+/dnaBkBAEDGcXWLTFIHDhwwY2QiIyMvOzhYFwAAkPkFNcicOXPGr3Vlz549smnTJilQoIBZtIuoffv2ZtaSjpEZMGCA3HTTTdKsWbNgFhsAALhEUIPM+vXrpUGDBt7b/fr1Mz87d+4sEyZMkC1btsjkyZMlNjbWXDSvadOm8tJLL9HiAgAAgh9k6tevL47jpHj//PnzM7Q8AADALlYN9gUAAPBFkAEAAFkryOg06EBX2NWxLHofAACAa4PM3r175dKlS8nW6xV1Dx48mBblAgAASNvBvl9//bXfQNzw8HDvbQ02ixYtkujo6KvZJQAAQMYEmbZt25qfISEhZoq0rxw5cpgQ8/rrr6e+NAAAAOkVZBITE81P/RLHdevWma8LAAAAsOo6MnoFXgAAAGsviKfjYXQ5duyYt6XG46OPPkqLsgEAAKR9kNHvQHrxxRelVq1a5gscdcwMAACAFUFm4sSJEhMTI506dUr7EgEAAKTndWQuXLggdevWTc1DAQAAghtknnjiCZk6dWralQIAACCjupbi4+Plvffek4ULF0q1atXMNWR8jRkzJjW7BQAASP8gs2XLFqlRo4b5fevWrX73MfAXAAC4OsgsWbIk7UsCAACQEWNkAAAArG2RadCgwWW7kBYvXnwtZQIAAEi/IOMZH+Nx8eJF2bRpkxkvk/TLJAEAAFwVZMaOHRtw/bBhw+TMmTPXWiYAAICMHyPTsWNHvmcJAADYGWRWr14tuXLlSstdAgAApG3XUrt27fxuO44jhw8flvXr18uQIUNSs0sAAICMCTLh4eF+t7Nlyybly5c334jdtGnT1OwSAAAgY4LMpEmTUvMwAACA4AcZjw0bNsivv/5qfq9cubLcfPPNaVUuAACA9Akyx44dkw4dOsjSpUslX758Zl1sbKy5UN60adOkcOHCqdktAABA+s9a6t27t5w+fVq2bdsmf/31l1n0YnhxcXHy9NNPp2aXAAAAGdMiM2/ePFm4cKFUrFjRu65SpUoyfvx4BvsCAAB3t8gkJiZKjhw5kq3XdXofAACAa4NMw4YN5ZlnnpFDhw551x08eFD69u0rjRo1SsvyAQAApG2Qefvtt814mOjoaClTpoxZSpUqZda99dZbqdklAABAxoyRiYqKko0bN5pxMtu3bzfrdLxM48aNU7M7AACA9G+RWbx4sRnUqy0vISEh0qRJEzODSZdbb73VXEvmhx9+SF1JAAAA0jPIjBs3Trp16yZ58+YN+LUFTz75pIwZM+ZqywAAAJD+QWbz5s3SvHnzFO/Xqdd6tV8AAADXBZmjR48GnHbtERoaKsePH0+LcgEAAKRtkClevLi5gm9KtmzZIpGRkVezSwAAgIwJMi1btpQhQ4ZIfHx8svvOnz8vQ4cOldatW6e+NAAAAOk1/Xrw4MEyY8YMKVeunPTq1UvKly9v1usUbP16gkuXLsm///3vq9klAABAxgSZokWLyqpVq6RHjx4yaNAgcRzHrNep2M2aNTNhRrcBAABw5QXxSpYsKd9++62cPHlSdu3aZcJM2bJlJX/+/OlTQgAAgLS8sq/S4KIXwQMAALDqu5YAAADcgCADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWCuoQWb58uVyzz33SLFixSQkJERmzZrld7/jOPLCCy9IZGSk5M6dWxo3biw7d+4MWnkBAIC7BDXInD17VqpXry7jx48PeP/o0aPlzTfflIkTJ8qPP/4oefLkkWbNmkl8fHyGlxUAALhPaDCfvEWLFmYJRFtjxo0bJ4MHD5Y2bdqYdVOmTJGiRYualpsOHTpkcGkBAIDbuHaMzJ49e+TIkSOmO8kjPDxcateuLatXr07xcQkJCRIXF+e3AACAzMm1QUZDjNIWGF9623NfICNGjDCBx7NERUWle1kBAEBwuDbIpNagQYPk1KlT3mX//v3BLhIAAMhqQSYiIsL8PHr0qN96ve25L5CwsDDJmzev3wIAADIn1waZUqVKmcCyaNEi7zod76Kzl+rUqRPUsgEAAHcI6qylM2fOyK5du/wG+G7atEkKFCggJUqUkD59+sh//vMfKVu2rAk2Q4YMMdecadu2bTCLDQAAXCKoQWb9+vXSoEED7+1+/fqZn507d5aYmBgZMGCAudZM9+7dJTY2Vu644w6ZN2+e5MqVK4ilBgAAbhHi6AVbMjHtjtLZSzrwl/EyAAA3ix44V2yzd2SroJ6/XTtGBgAA4EoIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtQgyAADAWgQZAABgLVcHmWHDhklISIjfUqFChWAXCwAAuESouFzlypVl4cKF3tuhoa4vMgAAyCCuTwUaXCIiIoJdDAAA4EKu7lpSO3fulGLFiknp0qXlkUcekX379l12+4SEBImLi/NbAABA5uTqIFO7dm2JiYmRefPmyYQJE2TPnj1y5513yunTp1N8zIgRIyQ8PNy7REVFZWiZAQBAxglxHMcRS8TGxkrJkiVlzJgx0rVr1xRbZHTx0BYZDTOnTp2SvHnzZmBpAQC4OtED54pt9o5slS771fO3Nkhc6fzt+jEyvvLlyyflypWTXbt2pbhNWFiYWQAAQObn6q6lpM6cOSO7d++WyMjIYBcFAAC4gKuDTP/+/WXZsmWyd+9eWbVqldx3332SPXt2eeihh4JdNAAA4AKu7lo6cOCACS0nTpyQwoULyx133CFr1qwxvwMAALg6yEybNi3YRQAAAC7m6q4lAACAyyHIAAAAaxFkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYK3QYBcAAK5F9MC5Ypu9I1sFuwhApkGLDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1QoNdAJtFD5wrNto7slWwiwAAQJqgRQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaVgSZ8ePHS3R0tOTKlUtq164ta9euDXaRAACAC7g+yHz++efSr18/GTp0qGzcuFGqV68uzZo1k2PHjgW7aAAAIMhcH2TGjBkj3bp1k8cee0wqVaokEydOlOuuu04++uijYBcNAAAEmauDzIULF2TDhg3SuHFj77ps2bKZ26tXrw5q2QAAQPCFiov9+eefcunSJSlatKjfer29ffv2gI9JSEgwi8epU6fMz7i4uDQvX2LCObFRerwWQLDY+DnkM4iU8H5Ovl/HccTaIJMaI0aMkOHDhydbHxUVFZTyuFH4uGCXAMja+AwiMwlP5/fz6dOnJTw83M4gU6hQIcmePbscPXrUb73ejoiICPiYQYMGmcHBHomJifLXX39JwYIFJSQkJE2Tooaj/fv3S968eSUzyux1zOz1ywp1pH72y+x1pH6ppy0xGmKKFSt22e1cHWRy5swpNWvWlEWLFknbtm29wURv9+rVK+BjwsLCzOIrX7586VZGPXCZ8c2ZleqY2euXFepI/eyX2etI/VLnci0xVgQZpa0rnTt3llq1asltt90m48aNk7Nnz5pZTAAAIGtzfZB58MEH5fjx4/LCCy/IkSNHpEaNGjJv3rxkA4ABAEDW4/ogo7QbKaWupGDR7iu9SF/SbqzMJLPXMbPXLyvUkfrZL7PXkfqlvxDnSvOaAAAAXMrVF8QDAAC4HIIMAACwFkEGAABYiyADAACsRZARkWHDhpmr/vouFSpUuOxjvvzyS7NNrly5pGrVqvLtt9/63a9jqHXKeGRkpOTOndt80eXOnTvFljq+//77cuedd0r+/PnNouVfu3at3zZdunRJts/mzZuLDfWLiYlJtr0eS7cew6utX/369ZNtr0urVq1cefw8Dh48KB07djRX4tbXXD9b69evv+xjli5dKrfccouZNXHTTTeZY5vU+PHjJTo62hzj2rVrJ3svu7V+M2bMkCZNmkjhwoXNxcbq1Kkj8+fPv+b/v9xSPz12gd6neqkNNx6/1NQx0OdMl8qVK7vuGEZHRwcsa8+ePV19LiTI/I++qQ4fPuxdVqxYkeK2q1atkoceeki6du0qP/30k7nqsC5bt271bjN69Gh58803ZeLEifLjjz9Knjx5pFmzZhIfHy821FH/g9E6LlmyxHzTuF6CumnTpuZD7EtPfL77/Oyzz8SG+ik9Mfhu/8cff/jd77ZjeDX10xOg77b63tSv+7j//vtde/xOnjwp9erVkxw5csh3330nv/zyi7z++usmSKdkz549Jpw1aNBANm3aJH369JEnnnjC72T/+eefmwtr6hTRjRs3SvXq1c1xPHbsmLi9fsuXLzdBRk8OGzZsMPW85557zP871/Led0v9PHbs2OFX/iJFirju+KW2jm+88YZf3fRS/gUKFEj2WXTDMVy3bp1fGRYsWGDWJy2r686FOv06qxs6dKhTvXr1f7z9Aw884LRq1cpvXe3atZ0nn3zS/J6YmOhEREQ4r776qvf+2NhYJywszPnss88cG+qY1N9//+3ccMMNzuTJk73rOnfu7LRp08Zxg6ut36RJk5zw8PAU73fbMbzW4zd27Fhz/M6cOePK46eee+4554477riqxwwYMMCpXLmy37oHH3zQadasmff2bbfd5vTs2dN7+9KlS06xYsWcESNGOG6vXyCVKlVyhg8fnmbvjWDWb8mSJXr5D+fkyZMpbuOW45dWx3DmzJlOSEiIs3fvXtcdw6SeeeYZp0yZMub/QzefC2mR+R9t6tIvpipdurQ88sgjsm/fvhS31RYKbR7zpQlT13v+StSmUd9t9PsitEnUs43b65jUuXPn5OLFi+YviaQtN/rXU/ny5aVHjx5y4sQJsaV+Z86ckZIlS5rWpjZt2si2bdu897nxGF7L8fvwww+lQ4cO5q8htx6/r7/+2nwVif71p2W6+eabTRfn5Vzps3jhwgXTkuG7TbZs2cztjD6OqalfUvpdc/olekk/h9fy3nBD/fSK7dr1oK1PK1eu9K530/FLq2Oon0Utv/7f47Zj6Etf+08++UQef/zxFL9w2S3nQoKMiHlRtV9dv/pgwoQJ5sXX8SH6H0YgemCSfkWC3vb063p+Xm4bt9cxqeeee858yHzfkNotMWXKFPMlnqNGjZJly5ZJixYt5NKlS+L2+umJ+6OPPpLZs2ebD6ueIOrWrSsHDhxw5TG8luOn4wm0qVe7XHy56fip33//3dStbNmypmtIg9XTTz8tkydPTvExKX0W9Rt5z58/L3/++aepjxuOY2rql9Rrr71mAvgDDzyQZp/tYNZPw4t2OXz11Vdm0T8qdHyXdiEpNx2/tDiGhw4dMl1SST+LbjmGvmbNmiWxsbFmjE9KXHMuTLO2nUxEmznz5s3rfPDBBwHvz5EjhzN16lS/dePHj3eKFClifl+5cqVpLj106JDfNvfff79pirOhjr60CTd//vzO5s2bL7vd7t27Tb0XLlzo2FQ/deHCBdOEOnjwYCuO4dXUr3v37k7VqlWvuF2wj59+rurUqeO3rnfv3s7tt9+e4mPKli3rvPLKK37r5s6da+px7tw55+DBg+b3VatW+W3zf//3f6bLwu318/Xpp5861113nbNgwYI0fe+7pX4ed911l9OxY0fzu5uOX1rUUd+rBQsWdBISElx5DH01bdrUad26tXM5bjkX0iITQL58+aRcuXKya9eugPdHRETI0aNH/dbpbV3vud+zLqVt3F5H378AR44cKd9//71Uq1btsttqk2ihQoWuuE831c9DB+9pM7Fne7cfw39aP/2m+GnTppnBeFcS7OOnf51XqlTJb13FihUv28Se0mdRB3LrDAmtjw5ydsNxTE39PPQY6l/xX3zxRbKm/Gt977uhfr5uu+02b9nddPyutY46e0dbgTt16iQ5c+Z05TH00IkPCxcuTNZy5NZzIUEmAG263b17t3nTBqJTILU53peO7tb1qlSpUuYg+W6jTd06Ytuzjdvr6Blt/tJLL5nmTu0XvhLtltExFpfbp5vq50ubr3/++Wfv9m4/hv+0fjo1MiEhwUwXdfvx09kgOnvF12+//ZZsLMHVfBb1hFGzZk2/bbQbUW9n9HFMTf2UziR77LHHzE/f6fNp9d4Pdv2S0tlnnrK76fhdax2161aDyT/5oyJYx9Bj0qRJZgzQld5vrjkXplnbjsWeffZZZ+nSpc6ePXtMU1jjxo2dQoUKOceOHTP3d+rUyRk4cKB3e90mNDTUee2115xff/3VjDjXJraff/7Zu83IkSOdfPnyObNnz3a2bNliZoeUKlXKOX/+vBV11PLnzJnTmT59unP48GHvcvr0aXO//uzfv7+zevVqs0/tjrjllltMU398fLzr66ezPubPn2+6UzZs2OB06NDByZUrl7Nt2zZXHsOrrZ+HzrDQWTxJue34qbVr15rP1csvv+zs3LnT25XyySefeLfROmpdPX7//XezjXY16GdRm7WzZ8/uzJs3z7vNtGnTzCyJmJgY55dffjFdbXpcjxw54vr66Tb6GK2X7+dQZ3780/eGm+uns+lmzZplttf/P3WWTLZs2fy6N91y/FJbRw/tLtMZPYG45Rh6ZoWVKFHCzNBKyq3nQoLM/6ZrRkZGmhN38eLFze1du3Z577/77rvNVFVfX3zxhVOuXDnzGJ3+qf3yvnTa2ZAhQ5yiRYuaD2GjRo2cHTt2OLbUsWTJkqZvM+mib1Sl4w+0D7Vw4cLmjavbd+vWLSj/uaSmfn369DEfVt1ej1HLli2djRs3uvYYpuY9un37dnPMvv/++2T7c9vx8/jmm2+cKlWqmNe7QoUKznvvved3v9ZR65p0Cm+NGjXMa1O6dGkztT6pt956y3u8dWzFmjVrHBvqp78H+hz6HusrvTfcXL9Ro0aZsWn6R0SBAgWc+vXrO4sXL3bt8Uvte1SDZ+7cuZNt68ZjOH/+fPMeC/R/nVvPhSH6T9q17wAAAGQcxsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyADIEF26dJGQkBCz6Jd06vewDBgwQOLj4//R45cuXWoeGxsbm+5lBWCP0GAXAEDW0bx5c/OFdBcvXpQNGzZI586dTTgZNWpUhpZDn1/DFAD70SIDIMOEhYWZb8ONioqStm3bSuPGjc235Xq+1XjEiBGmpSZ37txSvXp1mT59urlv79690qBBA/N7/vz5TfjRFh4VHR0t48aN83ueGjVqyLBhw7y3dfsJEybIvffeK3ny5JGXX37Z3K/bffzxx2Yf4eHh0qFDBzl9+nQGviIArhVBBkBQbN26VVatWiU5c+Y0tzXETJkyRSZOnCjbtm2Tvn37SseOHWXZsmUm+Hz11Vdmux07dsjhw4fljTfeuKrn0+By3333yc8//yyPP/64Wbd7926ZNWuWzJkzxyz6XCNHjkyH2gJIL3QtAcgwGhauv/56+fvvvyUhIUGyZcsmb7/9tvn9lVdekYULF0qdOnXMtqVLl5YVK1bIu+++K3fffbcUKFDArC9SpIjky5fvqp/74Ycflscee8xvnbYCxcTEyA033GBud+rUSRYtWmRabADYgSADIMNo95B28Zw9e1bGjh0roaGh0r59e9MCc+7cOWnSpInf9hcuXJCbb745TZ67Vq1aydZpl5InxKjIyEg5duxYmjwfgIxBkAGQYXR8yk033WR+/+ijj8w4mA8//FCqVKli1s2dO1eKFy+ebFzN5WirjuM4yQbzBnrupJIO+NWxNNpKA8AeBBkAQaEB5Pnnn5d+/frJb7/9ZgLLvn37TDdSIJ6xNJcuXfJbX7hwYTNmxiMuLk727NmTzqUH4BYM9gUQNPfff79kz57djIPp37+/GeA7efJkMwh348aN8tZbb5nbqmTJkqbFRMfZHD9+XM6cOWPWN2zY0Mw8+uGHH8xAXp3SrfsEkDXQIgMgaHSMTK9evWT06NGmFUVbV3T20u+//24G9N5yyy2m1UZpl9Pw4cNl4MCBZtDuo48+agbqDho0yDy2devWZgr1Sy+9RIsMkIWEOEk7lwEAACxB1xIAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAAYqv/B+5gNpN5qglNAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Problem 1 — Pick-and-Place Robot as an MDP (Option B: Stochastic)\n",
    "# ---------------------------------------------------------------\n",
    "# This cell implements a small simulator to generate episodes and returns for the simplified 5-state, 5-action MDP.\n",
    "#\n",
    "# Resul :\n",
    "#   • Clear definitions of States, Actions, Transition Probabilities\n",
    "#   • Reward function that encourages \"fast and smooth\" completion\n",
    "#   • A simple hand-crafted policy\n",
    "#   • Episode runner + batch evaluation + quick summaries\n",
    "#\n",
    "#  local values :\n",
    "#   - the probabilities p_reach, p_nav, p_arrive, p_place\n",
    "#   - the reward weights / logic in reward_fn\n",
    "#   - the policy() function ( random or any other logic)\n",
    "#\n",
    "# Dependencies: only Python stdlib + pandas/numpy/matplotlib \n",
    "# ================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, List\n",
    "import random\n",
    "from math import isclose\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) MDP Building Blocks\n",
    "# -----------------------------\n",
    "\n",
    "# Discrete state space \n",
    "STATES = [\"Idle\", \"HoldObject\", \"Moving\", \"EndGoal\", \"Release\"]   # \"Release\" is terminal\n",
    "\n",
    "# Discrete action space (tasks)\n",
    "ACTIONS = [\"MoveTowardObject\", \"Pickup\", \"MoveToGoal\", \"Release\", \"Stop\"]\n",
    "\n",
    "# --- Transition probabilities (stochastic realism) ---\n",
    "# Intuition:\n",
    "#   p_reach : success reaching the object from \"Idle\" via MoveTowardObject\n",
    "#   p_nav   : progress while moving toward the goal\n",
    "#   p_arrive: being correctly aligned at the goal when releasing from \"Moving\"\n",
    "#   p_place : successful place when at the goal and releasing\n",
    "#\n",
    "#  different amounts of noise/failure.\n",
    "p_reach  = 0.90\n",
    "p_nav    = 0.90\n",
    "p_arrive = 0.95\n",
    "p_place  = 0.98\n",
    "\n",
    "\n",
    "# Transition kernel P(s'|s,a), represented as:\n",
    "#   P[state][action] = list of (probability, next_state) pairs\n",
    "# Each row must sum to 1.0.\n",
    "P: Dict[str, Dict[str, List[Tuple[float, str]]]] = {\n",
    "    \"Idle\": {\n",
    "        \"MoveTowardObject\": [(p_reach, \"HoldObject\"), (1 - p_reach, \"Idle\")],\n",
    "        # Invalid actions in this state lead to self-loop \n",
    "        \"Pickup\":           [(1.0, \"Idle\")],\n",
    "        \"MoveToGoal\":       [(1.0, \"Idle\")],\n",
    "        \"Release\":          [(1.0, \"Idle\")],\n",
    "        # Stop ends the episode (transition to terminal)\n",
    "        \"Stop\":             [(1.0, \"Release\")],\n",
    "    },\n",
    "    \"HoldObject\": {\n",
    "        # Mostly progress to \"Moving\", with small chance of no progress or drop-back to Idle\n",
    "        \"MoveToGoal\":       [(p_nav, \"Moving\"), (0.08, \"HoldObject\"), (0.02, \"Idle\")],\n",
    "        # Already holding, \"Pickup\" has no effect (self-loop)\n",
    "        \"Pickup\":           [(1.0, \"HoldObject\")],\n",
    "        # Release here means we let go before moving—fall back to Idle most of the time\n",
    "        \"Release\":          [(0.98, \"Idle\"), (0.02, \"HoldObject\")],\n",
    "        # No reason to \"MoveTowardObject\" again (self-loop)\n",
    "        \"MoveTowardObject\": [(1.0, \"HoldObject\")],\n",
    "        \"Stop\":             [(1.0, \"Release\")],\n",
    "    },\n",
    "    \"Moving\": {\n",
    "        # Keep moving with high probability; sometimes slip back\n",
    "        \"MoveToGoal\":       [(p_nav, \"Moving\"), (0.05, \"HoldObject\"), (0.05, \"Idle\")],\n",
    "        # Try to release: usually means we've aligned at goal; small chance we're not there yet\n",
    "        \"Release\":          [(p_arrive, \"EndGoal\"), (1 - p_arrive, \"Moving\")],\n",
    "        \"Pickup\":           [(1.0, \"Moving\")],          # no effect while moving\n",
    "        \"MoveTowardObject\": [(1.0, \"Moving\")],          # no effect while moving\n",
    "        \"Stop\":             [(1.0, \"Release\")],\n",
    "    },\n",
    "    \"EndGoal\": {\n",
    "        # At the goal: releasing typically places successfully and ends the episode\n",
    "        \"Release\":          [(p_place, \"Release\"), (1 - p_place, \"EndGoal\")],\n",
    "        # Other actions are irrelevant here—self-loops\n",
    "        \"MoveToGoal\":       [(1.0, \"EndGoal\")],\n",
    "        \"MoveTowardObject\": [(1.0, \"EndGoal\")],\n",
    "        \"Pickup\":           [(1.0, \"EndGoal\")],\n",
    "        \"Stop\":             [(1.0, \"Release\")],\n",
    "    },\n",
    "    \"Release\": {\n",
    "        # Terminal: no further transitions (episode ends)\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "# Helper: a quick sanity check to ensure all rows sum to ~1.0\n",
    "def _check_transition_sums(P: Dict[str, Dict[str, List[Tuple[float, str]]]]):\n",
    "    for s, a_dict in P.items():\n",
    "        for a, pairs in a_dict.items():\n",
    "            total = sum(p for p, _ in pairs)\n",
    "            if not isclose(total, 1.0, rel_tol=1e-9, abs_tol=1e-9):\n",
    "                raise ValueError(f\"Transition probs for ({s}, {a}) sum to {total:.6f}, not 1.0\")\n",
    "_check_transition_sums(P)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Validity + Rewards\n",
    "# -----------------------------\n",
    "\n",
    "def is_terminal(state: str) -> bool:\n",
    "    \"\"\"Terminal check: episode ends when we reach 'Release'.\"\"\"\n",
    "    return state == \"Release\"\n",
    "\n",
    "def is_invalid_action(state: str, action: str) -> bool:\n",
    "    \"\"\"\n",
    "    Define which actions are 'invalid' in each state (for reward penalties).\n",
    "    We still allow the transition (often self-loop), but penalize the attempt.\n",
    "    \"\"\"\n",
    "    if state == \"Idle\" and action in (\"Pickup\", \"MoveToGoal\", \"Release\"):\n",
    "        return True\n",
    "    if state == \"HoldObject\" and action in (\"MoveTowardObject\",):\n",
    "        return True\n",
    "    if state == \"Moving\" and action in (\"MoveTowardObject\", \"Pickup\"):\n",
    "        return True\n",
    "    if state == \"EndGoal\" and action in (\"MoveToGoal\", \"MoveTowardObject\", \"Pickup\"):\n",
    "        return True\n",
    "    if state == \"Release\":\n",
    "        return True  # shouldn't act in terminal\n",
    "    return False\n",
    "\n",
    "def reward_fn(s: str, a: str, s_next: str) -> float:\n",
    "    \"\"\"\n",
    "    Reward shaping aligned with 'fast and smooth' goals:\n",
    "      +10 for successful place (EndGoal --Release--> Release)\n",
    "      -5  for invalid actions (trying things that don't make sense in this state)\n",
    "      -1  per time step otherwise (to encourage speed)\n",
    "    \"\"\"\n",
    "    # Success bonus: placing at the goal ends the episode\n",
    "    if s == \"EndGoal\" and a == \"Release\" and s_next == \"Release\":\n",
    "        return 10.0\n",
    "    # Penalize invalid attempts (keeps policy sensible)\n",
    "    if is_invalid_action(s, a):\n",
    "        return -5.0\n",
    "    # Small per-step time penalty\n",
    "    return -1.0\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Policy + Dynamics Sampler\n",
    "# -----------------------------\n",
    "\n",
    "def policy(state: str, mode: str = \"handcrafted\") -> str:\n",
    "    \"\"\"\n",
    "    Simple action selection.\n",
    "      - \"handcrafted\": a sensible, greedy task policy\n",
    "      - \"random\": sample any available action uniformly\n",
    "    \"\"\"\n",
    "    if mode == \"random\":\n",
    "        # Choose uniformly among all defined actions for this state\n",
    "        return random.choice(list(P.get(state, {}).keys())) if state in P else \"Stop\"\n",
    "\n",
    "    # Hand-crafted 'reasonable' mapping for this task:\n",
    "    if state == \"Idle\":\n",
    "        return \"MoveTowardObject\"\n",
    "    if state == \"HoldObject\":\n",
    "        return \"MoveToGoal\"\n",
    "    if state == \"Moving\":\n",
    "        return \"Release\"\n",
    "    if state == \"EndGoal\":\n",
    "        return \"Release\"\n",
    "    # For terminal or unknowns\n",
    "    return \"Stop\"\n",
    "\n",
    "def sample_next_state(state: str, action: str) -> str:\n",
    "    \"\"\"\n",
    "    Sample s' ~ P(s'|s,a) using the categorical distribution defined in P.\n",
    "    If no transitions exist (e.g., terminal), we return the same state.\n",
    "    \"\"\"\n",
    "    outcomes = P.get(state, {}).get(action, [])\n",
    "    if not outcomes:\n",
    "        return state  # No defined transition; stay put\n",
    "    r = random.random()\n",
    "    cum = 0.0\n",
    "    for prob, ns in outcomes:\n",
    "        cum += prob\n",
    "        if r <= cum or isclose(cum, 1.0, rel_tol=1e-12, abs_tol=1e-12):\n",
    "            return ns\n",
    "    # Numerical fallback\n",
    "    return outcomes[-1][1]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Episode + Batch Runners\n",
    "# -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class Step:\n",
    "    s: str\n",
    "    a: str\n",
    "    s_next: str\n",
    "    r: float\n",
    "\n",
    "@dataclass\n",
    "class EpisodeResult:\n",
    "    trace: List[Step]\n",
    "    total_return: float\n",
    "    steps: int\n",
    "    success: bool  # True iff we placed successfully (EndGoal --Release--> Release)\n",
    "\n",
    "def run_episode(max_steps: int = 100, policy_mode: str = \"handcrafted\", seed: int | None = None) -> EpisodeResult:\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    s = \"Idle\"\n",
    "    total = 0.0\n",
    "    trace: List[Step] = []\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        if is_terminal(s):\n",
    "            break\n",
    "\n",
    "        a = policy(s, mode=policy_mode)\n",
    "        s_next = sample_next_state(s, a)\n",
    "        r = reward_fn(s, a, s_next)\n",
    "\n",
    "        total += r\n",
    "        trace.append(Step(s=s, a=a, s_next=s_next, r=r))\n",
    "        s = s_next\n",
    "\n",
    "        if is_terminal(s):\n",
    "            break\n",
    "\n",
    "    # Success criterion: at some step we performed EndGoal --Release--> Release\n",
    "    success = any(step.s == \"EndGoal\" and step.a == \"Release\" and step.s_next == \"Release\" for step in trace)\n",
    "    return EpisodeResult(trace=trace, total_return=total, steps=len(trace), success=success)\n",
    "\n",
    "def run_n_episodes(n: int = 30, max_steps: int = 100, policy_mode: str = \"handcrafted\", seed: int = 123):\n",
    "    random.seed(seed)\n",
    "    results = [run_episode(max_steps=max_steps, policy_mode=policy_mode) for _ in range(n)]\n",
    "\n",
    "    # Build a small summary table \n",
    "    df = pd.DataFrame({\n",
    "        \"episode\": np.arange(1, n + 1),\n",
    "        \"steps\":   [r.steps for r in results],\n",
    "        \"return\":  [r.total_return for r in results],\n",
    "        \"success\": [r.success for r in results],\n",
    "    })\n",
    "    return results, df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Demo / Reporting\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Try one episode and print its step-by-step trace\n",
    "    ep = run_episode(max_steps=50, policy_mode=\"handcrafted\", seed=7)\n",
    "    print(\"First Episode Trace (s, a, s', r):\")\n",
    "    for step in ep.trace:\n",
    "        print(f\"({step.s:10s}, {step.a:16s}) -> ({step.s_next:10s})   r = {step.r:+.1f}\")\n",
    "    print(f\"\\nFirst Episode: steps = {ep.steps}, return = {ep.total_return:.1f}, success = {ep.success}\")\n",
    "\n",
    "    # Run a small batch of episodes and summarize\n",
    "    results, df = run_n_episodes(n=30, max_steps=50, policy_mode=\"handcrafted\", seed=7)\n",
    "    display(df.head(10))  \n",
    "    print(\"\\nSummary:\")\n",
    "    print(df.describe(include='all'))\n",
    "\n",
    "    # Quick histogram of returns \n",
    "    plt.figure()\n",
    "    plt.hist(df[\"return\"], bins=10)\n",
    "    plt.title(\"Episode Returns\")\n",
    "    plt.xlabel(\"Return\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c97ec5",
   "metadata": {},
   "source": [
    "### Problem 2 — 2×2 Gridworld (Value Iteration) — Cell 1/2\n",
    "####  Iteration 1**\n",
    "\n",
    "### Problem Setup\n",
    "- **States:**  \n",
    "  $$\n",
    "  S = \\{s_1, s_2, s_3, s_4\\}\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7512faad",
   "metadata": {},
   "source": [
    "\n",
    "- **Actions:**  \n",
    "  $$\n",
    "  A = \\{\\text{up}, \\text{down}, \\text{left}, \\text{right}\\}\n",
    "  $$\n",
    "\n",
    "- **Transitions:** Deterministic. If an action would leave the grid → the agent **stays** in the same state.  \n",
    "\n",
    "- **Rewards (state-based):**  \n",
    "  $$\n",
    "  R(s_1)=5, \\quad R(s_2)=10, \\quad R(s_3)=1, \\quad R(s_4)=2\n",
    "  $$\n",
    "\n",
    "- **Discount factor:**  \n",
    "  $$\n",
    "  \\gamma = 1\n",
    "  $$\n",
    "\n",
    "- **Initial values (convention):**  \n",
    "  $$\n",
    "  V^{(0)}(s)=0 \\quad \\forall s \\in S\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### Bellman Optimality Update\n",
    "For deterministic transitions, state-based rewards, and \\(\\gamma=1\\):\n",
    "\n",
    "$$\n",
    "V^{(k+1)}(s) = \\max_{a \\in A} \\Big[ R(s) + V^{(k)}(s'(s,a)) \\Big]\n",
    "$$\n",
    "\n",
    "> *In words:* “The new value of a state equals the reward of that state plus the **largest old value** among the states you can reach.”\n",
    "\n",
    "---\n",
    "\n",
    "### Iteration 1\n",
    "Since \\(V^{(0)}(s)=0\\) for all states:\n",
    "\n",
    "$$\n",
    "V^{(1)}(s) = R(s)\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "V^{(1)}(s_1)=5, \\quad V^{(1)}(s_2)=10, \\quad V^{(1)}(s_3)=1, \\quad V^{(1)}(s_4)=2\n",
    "$$\n",
    "\n",
    "**Grid after Iteration 1**\n",
    "| s1 = 5 | s2 = 10 |\n",
    "|---------|---------|\n",
    "| s3 = 1  | s4 = 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d86eb",
   "metadata": {},
   "source": [
    "## Problem 2 — 2×2 Gridworld (Value Iteration) — Cell 2/2\n",
    "### **Iteration 2**\n",
    "\n",
    "We now compute:\n",
    "\n",
    "$$\n",
    "V^{(2)} \\;\\; \\text{using} \\;\\; V^{(1)}.\n",
    "$$\n",
    "\n",
    "Neighborhoods:  \n",
    "- \\(s_1\\): Right → \\(s_2\\), Down → \\(s_3\\), Up/Left → stay in \\(s_1\\)  \n",
    "- \\(s_2\\): Left → \\(s_1\\), Down → \\(s_4\\), Up/Right → stay in \\(s_2\\)  \n",
    "- \\(s_3\\): Up → \\(s_1\\), Right → \\(s_4\\), Left/Down → stay in \\(s_3\\)  \n",
    "- \\(s_4\\): Up → \\(s_2\\), Left → \\(s_3\\), Right/Down → stay in \\(s_4\\)  \n",
    "\n",
    "Recall values from Iteration 1:\n",
    "\n",
    "$$\n",
    "V^{(1)}(s_1)=5, \\quad V^{(1)}(s_2)=10, \\quad V^{(1)}(s_3)=1, \\quad V^{(1)}(s_4)=2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed Updates\n",
    "\n",
    "**1) State \\(s_1\\), reward = 5**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Right: } & 5 + V^{(1)}(s_2) = 5 + 10 = 15 \\\\\n",
    "\\text{Down: }  & 5 + V^{(1)}(s_3) = 5 + 1  = 6 \\\\\n",
    "\\text{Up/Left: } & 5 + V^{(1)}(s_1) = 5 + 5  = 10 \\\\\n",
    "\\end{aligned}\n",
    "$$  \n",
    "\n",
    "$$\n",
    "V^{(2)}(s_1) = 15\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**2) State \\(s_2\\), reward = 10**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Left: } & 10 + V^{(1)}(s_1) = 10 + 5 = 15 \\\\\n",
    "\\text{Down: } & 10 + V^{(1)}(s_4) = 10 + 2 = 12 \\\\\n",
    "\\text{Up/Right: } & 10 + V^{(1)}(s_2) = 10 + 10 = 20 \\\\\n",
    "\\end{aligned}\n",
    "$$  \n",
    "\n",
    "$$\n",
    "V^{(2)}(s_2) = 20\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**3) State \\(s_3\\), reward = 1**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Up: } & 1 + V^{(1)}(s_1) = 1 + 5 = 6 \\\\\n",
    "\\text{Right: } & 1 + V^{(1)}(s_4) = 1 + 2 = 3 \\\\\n",
    "\\text{Left/Down: } & 1 + V^{(1)}(s_3) = 1 + 1 = 2 \\\\\n",
    "\\end{aligned}\n",
    "$$  \n",
    "\n",
    "$$\n",
    "V^{(2)}(s_3) = 6\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**4) State \\(s_4\\), reward = 2**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Up: } & 2 + V^{(1)}(s_2) = 2 + 10 = 12 \\\\\n",
    "\\text{Left: } & 2 + V^{(1)}(s_3) = 2 + 1  = 3 \\\\\n",
    "\\text{Right/Down: } & 2 + V^{(1)}(s_4) = 2 + 2 = 4 \\\\\n",
    "\\end{aligned}\n",
    "$$  \n",
    "\n",
    "$$\n",
    "V^{(2)}(s_4) = 12\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Grid after Iteration 2\n",
    "| s1 = 15 | s2 = 20 |\n",
    "|---------|---------|\n",
    "| s3 = 6  | s4 = 12 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
