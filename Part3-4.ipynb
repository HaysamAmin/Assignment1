{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60402f5c",
   "metadata": {},
   "source": [
    "### CSCN8020-25F-Sec1-Reinforcement Learning Programming\n",
    "### Assignment one \n",
    "#### Name : Haysam Elamin\n",
    "#### StID : 8953681"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952278e1",
   "metadata": {},
   "source": [
    "## Problem 3 — 5×5 Gridworld \n",
    "\n",
    "### MDP Setup\n",
    "- **States:**  \n",
    "  $$\n",
    "  S = \\{s_{i,j} \\mid i,j \\in \\{0,1,2,3,4\\}\\}\n",
    "  $$\n",
    "  Each cell in the 5×5 grid is a state, identified by its row and column.\n",
    "\n",
    "- **Terminal/Goal state:**  \n",
    "  $$\n",
    "  s_{\\text{goal}} = s_{4,4}\n",
    "  $$\n",
    "  Entering this state ends the episode.\n",
    "\n",
    "- **Grey states (bad states):**  \n",
    "  $$\n",
    "  G = \\{s_{1,2},\\; s_{3,0},\\; s_{0,4}\\}\n",
    "  $$\n",
    "\n",
    "- **Actions:**  \n",
    "  $$\n",
    "  A = \\{\\text{right}, \\text{left}, \\text{down}, \\text{up}\\}\n",
    "  $$\n",
    "\n",
    "- **Transition model:**  \n",
    "  If the action is valid, the transition is deterministic:\n",
    "  $$\n",
    "  P(s'|s,a) = 1 \\quad \\text{if } s' \\text{ is the neighbor from action } a\n",
    "  $$\n",
    "  If invalid (outside the grid), then:\n",
    "  $$\n",
    "  s' = s\n",
    "  $$\n",
    "\n",
    "- **Rewards:**  \n",
    "  $$\n",
    "  R(s) =\n",
    "  \\begin{cases}\n",
    "  +10 & \\text{if } s = s_{4,4} \\\\\n",
    "  -5 & \\text{if } s \\in \\{s_{1,2}, s_{3,0}, s_{0,4}\\} \\\\\n",
    "  -1 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### Bellman Optimality Update\n",
    "The value iteration update rule is:\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) =\n",
    "\\begin{cases}\n",
    "R(s) & \\text{if } s \\text{ is terminal} \\\\\n",
    "\\max\\limits_{a \\in A} \\Big[ R(s) + \\gamma \\cdot V_k(s'(s,a)) \\Big], & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The greedy policy is then extracted as:\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_{a \\in A} \\Big[ R(s) + \\gamma \\cdot V(s'(s,a)) \\Big]\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4360c42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Function R(s) for the 5x5 Gridworld:\n",
      "[[-1. -1. -1. -1. -5.]\n",
      " [-1. -1. -5. -1. -1.]\n",
      " [-1. -1. -1. -1. -1.]\n",
      " [-5. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. 10.]]\n",
      "\n",
      "Formatted Reward Grid (row x col):\n",
      " -1.0 |  -1.0 |  -1.0 |  -1.0 |  X(-5) \n",
      " -1.0 |  -1.0 |  X(-5)  |  -1.0 |  -1.0\n",
      " -1.0 |  -1.0 |  -1.0 |  -1.0 |  -1.0\n",
      " X(-5)  |  -1.0 |  -1.0 |  -1.0 |  -1.0\n",
      " -1.0 |  -1.0 |  -1.0 |  -1.0 |  G(+10) \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 1: Define the reward function R(s) for the 5x5 Gridworld.\n",
    "\n",
    "Rules:\n",
    "- Goal state (4,4) has reward +10\n",
    "- Grey states [(1,2), (3,0), (0,4)] have reward -5\n",
    "- All other states have reward -1\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: Define grid size\n",
    "# ------------------------------\n",
    "n = 5   # This is a 5x5 Gridworld, so we have 25 states in total\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: Define special states\n",
    "# ------------------------------\n",
    "goal_state = (4, 4)             # The terminal/goal state (bottom-right corner)\n",
    "grey_states = [(1, 2), (3, 0), (0, 4)]  # The \"bad\" states (non-terminal but penalized)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: Initialize rewards\n",
    "# ------------------------------\n",
    "# Start by assigning -1 (step penalty) to ALL states.\n",
    "# This encourages the agent to reach the goal in the fewest steps possible.\n",
    "R = -1 * np.ones((n, n))\n",
    "\n",
    "# ------------------------------\n",
    "# Step 4: Update reward for goal state\n",
    "# ------------------------------\n",
    "# Assign +10 reward to the goal state.\n",
    "# This makes it attractive for the agent to navigate towards.\n",
    "R[goal_state] = 10\n",
    "\n",
    "# ------------------------------\n",
    "# Step 5: Update rewards for grey states\n",
    "# ------------------------------\n",
    "# These are \"non-favorable\" states that punish the agent with -5 if entered.\n",
    "for g in grey_states:\n",
    "    R[g] = -5\n",
    "\n",
    "# ------------------------------\n",
    "# Step 6: Display the reward function\n",
    "# ------------------------------\n",
    "# This prints the reward grid in matrix form\n",
    "# where each entry corresponds to the reward of that state.\n",
    "print(\"Reward Function R(s) for the 5x5 Gridworld:\")\n",
    "print(R)\n",
    "\n",
    "# For clarity, let’s also print it in a grid layout with labels\n",
    "print(\"\\nFormatted Reward Grid (row x col):\")\n",
    "for i in range(n):\n",
    "    row_repr = []\n",
    "    for j in range(n):\n",
    "        if (i, j) == goal_state:\n",
    "            row_repr.append(\" G(+10) \")\n",
    "        elif (i, j) in grey_states:\n",
    "            row_repr.append(\" X(-5) \")\n",
    "        else:\n",
    "            row_repr.append(f\"{R[i, j]:>5}\")\n",
    "    print(\" | \".join(row_repr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a473c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcce482a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env size: 5\n",
      "is terminal(4,4): True\n",
      "Agent created. V shape: (5, 5)\n",
      "Agent.calculate_max_value(0,0) -> -1.0 0\n"
     ]
    }
   ],
   "source": [
    "from value_iteration import GridWorld\n",
    "from value_iteration_agent import Agent\n",
    "\n",
    "env = GridWorld(5)\n",
    "print(\"env size:\", env.get_size())\n",
    "print(\"is terminal(4,4):\", env.is_terminal_state(4,4))\n",
    "\n",
    "agent = Agent(env, theta_threshold=0.05)\n",
    "print(\"Agent created. V shape:\", agent.get_value_function().shape)\n",
    "\n",
    "# probe one Bellman update at a non-terminal state:\n",
    "try:\n",
    "    v, a, _ = agent.calculate_max_value(0,0)\n",
    "    print(\"Agent.calculate_max_value(0,0) ->\", v, a)\n",
    "except Exception as e:\n",
    "    print(\"Agent.calculate_max_value error:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbdbe5a",
   "metadata": {},
   "source": [
    "#### 1 — Import, configure, and run Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38ef7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 10 iteration(s) (or hit cap 1000).\n",
      "V* (numpy array):\n",
      " [[ 0.95011888  2.05275672  3.21342812  4.4351875   5.72125   ]\n",
      " [ 2.05275672  3.21342812  4.4351875   5.72125     7.075     ]\n",
      " [ 3.21342812  4.4351875   5.72125     7.075       8.5       ]\n",
      " [ 4.4351875   5.72125     7.075       8.5        10.        ]\n",
      " [ 5.72125     7.075       8.5        10.          0.        ]]\n",
      "\n",
      "Greedy policy π*:\n",
      "['Right|Down', 'Right|Down', 'Right', 'Down', 'Down']\n",
      "['Right|Down', 'Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right', 'Right', 'Right', 'Right', 'X']\n"
     ]
    }
   ],
   "source": [
    "# --- Part 2: Run Value Iteration and obtain V* and π* ---\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from value_iteration import GridWorld\n",
    "from value_iteration_agent import Agent\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Configure environment/agent\n",
    "# -----------------------------\n",
    "ENV_SIZE = 5\n",
    "THETA_THRESHOLD = 0.05     # convergence tolerance; lower = more precise, more iters\n",
    "MAX_ITERATIONS = 1000\n",
    "\n",
    "env = GridWorld(ENV_SIZE)\n",
    "agent = Agent(env, THETA_THRESHOLD)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Value Iteration loop \n",
    "# -----------------------------\n",
    "done = False\n",
    "for it in range(MAX_ITERATIONS):\n",
    "    if done:\n",
    "        break\n",
    "    V_old = agent.get_value_function()\n",
    "    new_V = np.copy(V_old)\n",
    "\n",
    "    # Bellman optimality update for every non-terminal state\n",
    "    for i in range(ENV_SIZE):\n",
    "        for j in range(ENV_SIZE):\n",
    "            if not env.is_terminal_state(i, j):\n",
    "                # calculate_max_value returns (best_value, best_action, q_values) \n",
    "                best_v, _, _ = agent.calculate_max_value(i, j)\n",
    "                new_V[i, j] = best_v\n",
    "\n",
    "    # Convergence check: Δ = max |V_new - V_old|\n",
    "    done = agent.is_done(new_V)\n",
    "    agent.update_value_function(new_V)\n",
    "\n",
    "print(f\"Converged in {it+1} iteration(s) (or hit cap {MAX_ITERATIONS}).\")\n",
    "V_star = agent.get_value_function()\n",
    "\n",
    "# Derive the greedy policy π* from V*\n",
    "agent.update_greedy_policy()\n",
    "print(\"V* (numpy array):\\n\", V_star)\n",
    "print(\"\\nGreedy policy π*:\")\n",
    "agent.print_policy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce2eab",
   "metadata": {},
   "source": [
    "#### figures: 5×5 tables for \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2e14c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Figure: Optimal State-Values V*\n",
      "\n",
      "| c0 | c1 | c2 | c3 | c4 |\n",
      "|---|---|---|---|---|\n",
      "|  0.95|  2.05|  3.21|  4.44|  5.72|\n",
      "|  2.05|  3.21|  4.44|  5.72|  7.07|\n",
      "|  3.21|  4.44|  5.72|  7.07|  8.50|\n",
      "|  4.44|  5.72|  7.07|  8.50| 10.00|\n",
      "|  5.72|  7.07|  8.50| 10.00|  0.00|\n",
      "\n",
      "### Figure: Optimal Policy π* (arrows)\n",
      "\n",
      "| c0 | c1 | c2 | c3 | c4 |\n",
      "|---|---|---|---|---|\n",
      "|      |      |      |      |      |\n",
      "|      |      |      |      |      |\n",
      "|      |      |      |      |      |\n",
      "|      |      |      |      |      |\n",
      "|      |      |      |      |      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper: print a Markdown table from a 2D list of strings\n",
    "def print_markdown_table(title, rows):\n",
    "    print(f\"### {title}\\n\")\n",
    "    n = len(rows[0])\n",
    "    header = \"|\" + \"|\".join([f\" c{j} \" for j in range(n)]) + \"|\"\n",
    "    sep    = \"|\" + \"|\".join([\"---\" for _ in range(n)]) + \"|\"\n",
    "    print(header)\n",
    "    print(sep)\n",
    "    for r in rows:\n",
    "        print(\"|\" + \"|\".join(r) + \"|\")\n",
    "    print()\n",
    "\n",
    "# 1) Values table (rounded)\n",
    "vals_str = [[f\"{V_star[i,j]:6.2f}\" for j in range(ENV_SIZE)] for i in range(ENV_SIZE)]\n",
    "print_markdown_table(\"Figure: Optimal State-Values V*\", vals_str)\n",
    "\n",
    "# 2) Policy table (arrows)\n",
    "# Many class templates store the greedy policy inside the agent; if not, adapt below.\n",
    "# Try to use agent.policy_grid if your Agent exposes it; otherwise, build from print_policy again.\n",
    "try:\n",
    "    pol = agent.policy_grid  # e.g., a 2D array of arrow strings like \"↑\", \"→\", \"↑→\" for ties\n",
    "except AttributeError:\n",
    "    # Fallback: derive a 2D policy grid from agent (common pattern)\n",
    "    # If your Agent has a method to return the greedy action per state, use it instead.\n",
    "    pol = np.full_like(V_star, \"\", dtype=object)\n",
    "    arrows = agent.get_greedy_policy_arrows() if hasattr(agent, \"get_greedy_policy_arrows\") else None\n",
    "    if arrows is not None:\n",
    "        pol = arrows\n",
    "    else:\n",
    "        # As a last resort, parse agent.print_policy() output is cumbersome;\n",
    "        # better to expose a policy grid in your Agent class.\n",
    "        print(\"Note: Expose a policy grid in Agent (e.g., agent.policy_grid) for a clean table.\")\n",
    "\n",
    "# If we have pol as a 2D array of strings, print it nicely:\n",
    "if isinstance(pol, np.ndarray) and pol.shape == V_star.shape:\n",
    "    pol_str = [[f\"{pol[i,j]:^6}\" for j in range(ENV_SIZE)] for i in range(ENV_SIZE)]\n",
    "    print_markdown_table(\"Figure: Optimal Policy π* (arrows)\", pol_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53840b6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "348ee42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class OffPolicyMonteCarloAgent:\n",
    "    def __init__(self, env, gamma=0.9, episodes=5000):\n",
    "        \"\"\"\n",
    "        Off-policy Monte Carlo agent with Importance Sampling.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env : GridWorld object\n",
    "            The environment (same as in Problem 3).\n",
    "        gamma : float\n",
    "            Discount factor (default = 0.9).\n",
    "        episodes : int\n",
    "            Number of episodes to run for training.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.episodes = episodes\n",
    "\n",
    "        # --- State-value function V(s) ---\n",
    "        # We keep estimates of value for each state (initialized to 0).\n",
    "        self.V = np.zeros((env.get_size(), env.get_size()))\n",
    "\n",
    "        # --- Cumulative weights C(s) ---\n",
    "        # Used for weighted importance sampling updates.\n",
    "        self.C = np.zeros((env.get_size(), env.get_size()))\n",
    "\n",
    "        # --- Target policy (π) ---\n",
    "        # We’ll store the greedy policy as action indices (0=→,1=←,2=↓,3=↑, -1=Goal).\n",
    "        self.policy = np.full((env.get_size(), env.get_size()), -1, dtype=int)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Behavior Policy b(a|s): Random uniform over 4 actions\n",
    "    # -------------------------------------------------------------\n",
    "    def behavior_policy(self, state):\n",
    "        \"\"\"Choose an action uniformly at random from {Right, Left, Down, Up}.\"\"\"\n",
    "        return np.random.choice(len(self.env.get_actions()))\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Generate one episode following the behavior policy\n",
    "    # -------------------------------------------------------------\n",
    "    def generate_episode(self):\n",
    "        \"\"\"\n",
    "        Generate a single episode:\n",
    "        - Start from a random non-terminal state\n",
    "        - Follow the behavior policy until reaching the goal (terminal)\n",
    "        Returns:\n",
    "            episode : list of (state, action, reward)\n",
    "        \"\"\"\n",
    "        env_size = self.env.get_size()\n",
    "        \n",
    "        # Pick a random starting position (not terminal)\n",
    "        i, j = np.random.randint(0, env_size), np.random.randint(0, env_size)\n",
    "        while self.env.is_terminal_state(i, j):\n",
    "            i, j = np.random.randint(0, env_size), np.random.randint(0, env_size)\n",
    "        \n",
    "        episode = []\n",
    "        done = False\n",
    "        \n",
    "        # Generate the episode\n",
    "        while not done:\n",
    "            a = self.behavior_policy((i, j))   # pick action from behavior policy\n",
    "            ni, nj, r, done = self.env.step(a, i, j)  # take step in env\n",
    "            episode.append(((i, j), a, r))     # record transition\n",
    "            i, j = ni, nj                      # move to next state\n",
    "        \n",
    "        return episode\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Off-Policy Monte Carlo with Importance Sampling\n",
    "    # -------------------------------------------------------------\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Run Monte Carlo updates for multiple episodes.\n",
    "        Uses Weighted Importance Sampling to update V(s).\n",
    "        \"\"\"\n",
    "        for ep in range(self.episodes):\n",
    "            episode = self.generate_episode()\n",
    "            G = 0.0     # return accumulator\n",
    "            W = 1.0     # importance sampling weight\n",
    "\n",
    "            # Process episode backwards (MC update)\n",
    "            for t in reversed(range(len(episode))):\n",
    "                (s_i, s_j), a, r = episode[t]\n",
    "                \n",
    "                # Update return\n",
    "                G = self.gamma * G + r\n",
    "\n",
    "                # Update cumulative weight for this state\n",
    "                self.C[s_i, s_j] += W\n",
    "\n",
    "                # Update V(s) with weighted average:\n",
    "                # V(s) <- V(s) + (W / C(s)) * (G - V(s))\n",
    "                self.V[s_i, s_j] += (W / self.C[s_i, s_j]) * (G - self.V[s_i, s_j])\n",
    "                \n",
    "                # Compute greedy action under current env dynamics (target policy π)\n",
    "                best_val, best_a, _ = self.env.calculate_max_value(s_i, s_j)\n",
    "\n",
    "                # If the action taken in the episode != greedy target policy → weight goes to 0\n",
    "                if a != best_a:\n",
    "                    break\n",
    "                \n",
    "                # Otherwise, multiply weight by 1/b(a|s).\n",
    "                # Since behavior policy is uniform random, b(a|s)=0.25 → multiply by 4.\n",
    "                W *= 1.0 / 0.25  \n",
    "\n",
    "        # After training, build the greedy target policy\n",
    "        for i in range(self.env.get_size()):\n",
    "            for j in range(self.env.get_size()):\n",
    "                if not self.env.is_terminal_state(i, j):\n",
    "                    _, best_a, _ = self.env.calculate_max_value(i, j)\n",
    "                    self.policy[i, j] = best_a\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Pretty-printing helpers\n",
    "    # -------------------------------------------------------------\n",
    "    def print_value_function(self):\n",
    "        \"\"\"Print rounded state-value function table.\"\"\"\n",
    "        print(\"\\nEstimated Value Function (Off-policy MC with IS):\")\n",
    "        print(np.round(self.V, 2))\n",
    "\n",
    "    def print_policy(self):\n",
    "        \"\"\"Print greedy policy arrows.\"\"\"\n",
    "        arrow = {0: \"→\", 1: \"←\", 2: \"↓\", 3: \"↑\", -1: \"G\"}\n",
    "        print(\"\\nGreedy Policy (π):\")\n",
    "        for i in range(self.env.get_size()):\n",
    "            print(\" \".join(f\"{arrow[self.policy[i,j]]:>2}\" for j in range(self.env.get_size())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e28ae4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b8da38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Value Function (Off-policy MC with IS):\n",
      "[[ 0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.  ]\n",
      " [ 3.12  4.58  6.2   8.   10.  ]\n",
      " [ 3.66  4.65  6.2   8.04  0.  ]]\n",
      "\n",
      "Greedy Policy (π):\n",
      " →  →  →  →  →\n",
      " →  →  →  →  →\n",
      " →  →  →  →  →\n",
      " →  →  →  →  →\n",
      " →  →  →  →  G\n",
      "\n",
      "Value Function from Value Iteration (Problem 3):\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      " →  →  →  →  →\n",
      " →  →  →  →  →\n",
      " →  →  →  →  →\n",
      " →  →  →  →  →\n",
      " →  →  →  →  G\n"
     ]
    }
   ],
   "source": [
    "from value_iteration import GridWorld\n",
    "\n",
    "# Create environment\n",
    "env = GridWorld(env_size=5, gamma=0.95)\n",
    "\n",
    "# Create Off-policy Monte Carlo agent\n",
    "mc_agent = OffPolicyMonteCarloAgent(env, gamma=0.9, episodes=10000)\n",
    "\n",
    "# Train agent\n",
    "mc_agent.update()\n",
    "\n",
    "# Print results\n",
    "mc_agent.print_value_function()\n",
    "mc_agent.print_policy()\n",
    "\n",
    "# For comparison, also show Value Iteration results\n",
    "print(\"\\nValue Function from Value Iteration (Problem 3):\")\n",
    "print(np.round(env.get_value_function(), 2))\n",
    "env.update_greedy_policy()\n",
    "env.print_policy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
